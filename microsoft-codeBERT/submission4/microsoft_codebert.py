# -*- coding: utf-8 -*-
"""microsoft-CodeBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QsENuwTOEIoYpR-nT5CR5dv3usUH7WK
"""

#optimization: label_num = 2
#model: microsoft/CodeBERT
#learning rate: 1e-5
#train_test_split=0.1, seed = 100
#batch_size =

!pip install torch

!pip install transformers

!pip install transformers datasets

import pandas as pd
import re
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, RobertaTokenizer, RobertaConfig, RobertaModel
from datasets import load_dataset
from pandas.core.common import random_state
import numpy as np
from datasets import load_metric
from transformers import TrainingArguments, Trainer
import logging
import sklearn.metrics as metric
from transformers import DataCollatorWithPadding
import torch,gc
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from torch.nn import CrossEntropyLoss

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Load Train dataset
train1 = pd.read_csv('sample_train.csv', encoding = 'utf-8')
df1 = pd.DataFrame(train1)

#Define Model

pretrained = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base")
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
#loss_fn = 

batch_size = 32
epoch_num = 10
MAX_LEN = 512
learning_rate = 1e-5

def preprocess(df):
  #preprocess_df = df.replace(re.compile('(^import.*|^from.*)',re.MULTILINE),"",regex=True) #import,from 없애기
  preprocess_df = df.replace(re.compile('(#.*)', re.MULTILINE),"",regex=True) #주석 한 줄
  preprocess_df = preprocess_df.replace(re.compile('[\'\"]{3}.*?[\'\"]{3}', re.DOTALL),"",regex=True) #주석 여러줄
  preprocess_df = preprocess_df.replace(re.compile('[\n]{2,}', re.MULTILINE),"\n",regex=True) #다중개행 한번으로
  preprocess_df = preprocess_df.replace(re.compile('[ ]{4}', re.MULTILINE),"\t",regex=True) #tab 변환
  preprocess_df = preprocess_df.replace(re.compile('[ ]{1,3}', re.MULTILINE)," ",regex=True) #공백 여러개 변환
  preprocess_df.to_csv('preprocess.csv')
  
def tokenized(examples):
  return tokenizer(examples['code1'],examples['code2'], padding=True, max_length=MAX_LEN,truncation=True, return_token_type_ids=True)

preprocess(df1)
dataset = load_dataset("csv",data_files="preprocess.csv")['train']
#truncation을 하는 대신에 왼쪽부터 자르도록 해야하나?
encoded_dataset = dataset.map(tokenized,remove_columns=['Unnamed: 0','code1','code2'],batched=True)
encoded_dataset=encoded_dataset.rename_column(original_column_name='similar',new_column_name='labels')
encoded_dataset
print(encoded_dataset[1])

print(tokenizer.tokenize(dataset['code2'][1]))

encoded_dataset = encoded_dataset.train_test_split(0.1,seed=100)

loss_fn = CrossEntropyLoss()#weight?

class CodeBERTModified():
  def __init__(self, my_pretrained_model):
    super(CodeBERTModified,self).__init__()
    self.pretrained = my_pretrained_model
    self.parameters = self.pretrained.parameters
    #self.grads = self.pretrained.grads
    self.cache = None
  
  def forward(self,input_ids=None,token_type_ids=None,labels=None):
    outputs = self.pretrained(input_ids, token_type_ids=token_type_ids, labels=labels)
    print(outputs)
    out = outputs[1]#의미하는게 뭐지?
    loss = loss_fn(outputs[1].view(-1,2),labels.view(-1))#여기도 어떻게 돌아가는건지
    return out,loss

  def backward(self):
    return 

  def predict(self,input_ids=None,token_type_ids=None,labels=None):
    return self.forward(input_ids, token_type_ids=token_type_ids, labels=labels)

model = CodeBERTModified(my_pretrained_model=pretrained)
#model.to(device)

train_dataset = encoded_dataset['train'][1]
print(train_dataset)

for step, batch in enumerate(encoded_dataset['train']):
    label = batch['labels']
    input_ids = batch['input_ids']
    token_ids = batch['token_type_ids']
    
    #model.zero_grads()

    logits, loss = model.forward(input_ids = np.array(input_ids), token_type_ids = np.array(token_ids), labels=np.array(label))
    break

optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)

def flat_accuracy(preds,labels): #이거는 metric accuracy로 대체 가능할듯
  pred_flat = np.argmax(preds, axis=1).flatten()
  labels_flat = labels.flatten()
  return np.sum(pred_flat == labels_flat) / len(labels_flat)

for epoch in range(epoch_num):

  #train
  print("======Epoch%d======",epoch)
  print("Traing...")

  #cuda memory error
  gc.collect()
  torch.cuda.empty_cache()

  total_train_loss = 0
  #model.train()

  for step, batch in enumerate(encoded_dataset['train']):
    label = batch['labels']
    input_ids = batch['input_ids']
    token_ids = batch['token_type_ids']
    
    #model.zero_grads()

    logits, loss = model.forward(input_ids = input_ids, token_type_ids = token_ids, labels=label)
    total_train_loss += loss.item()
    loss.backward()
    optimizer.step()

    if step % 50 == 0 and not step == 0:
      print('step : {:>5,} of {:>5,} loss: {:.5f}'.format(step, len(encoded_dataset['train']), loss.item()))

    avg_train_loss = total_train_loss / len(encoded_dataset['train'])
    print("")
    print("  Average training loss: {0:.5f}".format(avg_train_loss))


    #validation
    print("Validation...")

    model.eval()
    total_eval_accuracy = 0
    total_eval_loss=0

    for step,batch in enumerate(encoded_dataset['test']):
      label = batch[0]
      input_ids = batch[1]
      token_ids = batch[2]

      with torch.no_grads():
        logits, loss = model(input_ids = input_ids, token_type_ids = token_ids, labels=label)
        total_eval_loss += loss.item()
        logits = logits.detach().cpu().numpy()
        label_ids = label.to('cpu').numpy()
        total_eval_accuracy += flat_accuracy(logits, label_ids)


      avg_val_accuracy = total_eval_accuracy / len(encoded_dataset['test'])
      print("Accuracy: {0:.2f}".format(avg_val_accuracy))

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

metric = load_metric("accuracy")

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return metric.compute(predictions=predictions, references=labels)

metric_name = "accuracy"

args = TrainingArguments("test", save_strategy="epoch",evaluation_strategy="epoch",logging_strategy="epoch", 
                         learning_rate=1e-5,per_device_train_batch_size=batch_size,
                        per_device_eval_batch_size=batch_size,num_train_epochs=epoch_num,weight_decay=0.01,
                         do_train=True,do_eval=True,metric_for_best_model=metric_name,load_best_model_at_end=True)

trainer = Trainer(model,args,train_dataset=encoded_dataset['train'],eval_dataset=encoded_dataset['test'],
                  tokenizer=tokenizer, compute_metrics=compute_metrics, data_collator=data_collator)

#trainer.train()

#trainer.evaluate()

TEST = "test.csv"
SAMPLE = "sample_submission.csv"

test_dataset = load_dataset("csv",data_files = TEST)['train']
test_dataset = test_dataset.map(tokenized,remove_columns=['code1','code2'])

predictions = trainer.predict(test_dataset)

df = pd.read_csv(SAMPLE)
df['similar'] = np.argmax(predictions.predictions,axis=-1)
df.to_csv('submission.csv',index=False)