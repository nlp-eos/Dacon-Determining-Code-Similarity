# -*- coding: utf-8 -*-
"""klue-roberta base(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QsENuwTOEIoYpR-nT5CR5dv3usUH7WK
"""

# Preprocess DATA
import pandas as pd
import re

data = pd.read_csv('sample_train.csv', encoding = 'utf-8')
df = pd.DataFrame(data)
print(df)

preprocess_df = df.replace(re.compile('(^import.*|^from.*)',re.MULTILINE),"",regex=True) #import,from 없애기
preprocess_df = preprocess_df.replace(re.compile('(#.*)', re.MULTILINE),"",regex=True) #주석 한 줄
preprocess_df = preprocess_df.replace(re.compile('[\'\"]{3}.*?[\'\"]{3}', re.DOTALL),"",regex=True) #주석 여러줄
preprocess_df = preprocess_df.replace(re.compile('[\n]{2,}', re.MULTILINE),"\n",regex=True) #다중개행 한번으로
preprocess_df = preprocess_df.replace(re.compile('[ ]{4}', re.MULTILINE),"\t",regex=True) #tab 변환
preprocess_df = preprocess_df.replace(re.compile('[ ]{1,3}', re.MULTILINE)," ",regex=True) #공백 여러개 변환

print(preprocess_df)

preprocess_df.to_csv('preprocess.csv')

!pip install transformers

#Define Model

from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("klue/roberta-base")
tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")

batch_size = 40
epoch_num = 5
MAX_LEN = 256

#Tokenize

#preprocess_df['tokenized1'] = df.apply(lambda row: tokenizer.to(row['sentences']), axis=1)


def tokenized(data):
  return tokenizer(data, padding=True, max_length=MAX_LEN,truncation=True, return_token_type_ids=False)
#print(preprocess_df.map(tokenized(preprocess_df)))

for (code1,code2) in zip(preprocess_df['code1'], preprocess_df['code2']):
  preprocess_df['tokenized1'] = df.apply(lambda row: tokenized(code1), axis=1)
  print(preprocess_df['tokenized1'][1])
  print(preprocess_df)
  break