# -*- coding: utf-8 -*-
"""microsoft-CodeBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QsENuwTOEIoYpR-nT5CR5dv3usUH7WK
"""

#optimization: label_num = 2
#model: microsoft/CodeBERT
#learning rate: 1e-5
#train_test_split=0.1, seed = 100
#batch_size =

!pip install torch

!pip install transformers

!pip install transformers datasets

import pandas as pd
import re
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, RobertaTokenizer, RobertaConfig, RobertaModel
from datasets import load_dataset
from pandas.core.common import random_state
import numpy as np
from datasets import load_metric
from transformers import TrainingArguments, Trainer
import logging
import sklearn.metrics as metric
from transformers import DataCollatorWithPadding
import torch,gc
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#Load Train dataset
train1 = pd.read_csv('sample_train.csv', encoding = 'utf-8')
df1 = pd.DataFrame(train1)

#Define Model

pretrained = RobertaModel.from_pretrained("microsoft/codebert-base")
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
#loss_fn = 

batch_size = 32
epoch_num = 10
MAX_LEN = 512
learning_rate = 1e-5

def preprocess(df):
  #preprocess_df = df.replace(re.compile('(^import.*|^from.*)',re.MULTILINE),"",regex=True) #import,from 없애기
  preprocess_df = df.replace(re.compile('(#.*)', re.MULTILINE),"",regex=True) #주석 한 줄
  preprocess_df = preprocess_df.replace(re.compile('[\'\"]{3}.*?[\'\"]{3}', re.DOTALL),"",regex=True) #주석 여러줄
  preprocess_df = preprocess_df.replace(re.compile('[\n]{2,}', re.MULTILINE),"\n",regex=True) #다중개행 한번으로
  preprocess_df = preprocess_df.replace(re.compile('[ ]{4}', re.MULTILINE),"\t",regex=True) #tab 변환
  preprocess_df = preprocess_df.replace(re.compile('[ ]{1,3}', re.MULTILINE)," ",regex=True) #공백 여러개 변환
  preprocess_df.to_csv('preprocess.csv')
  
def tokenized(examples):
  return tokenizer(examples['code1'],examples['code2'], padding=True, max_length=MAX_LEN,truncation=False, return_token_type_ids=False)

preprocess(df1)
dataset = load_dataset("csv",data_files="preprocess.csv")['train']
encoded_dataset = dataset.map(tokenized,remove_columns=['Unnamed: 0','code1','code2'],batched=True)
encoded_dataset=encoded_dataset.rename_column(original_column_name='similar',new_column_name='labels')
encoded_dataset
print(encoded_dataset[1])

encoded_dataset = encoded_dataset.train_test_split(0.1,seed=100)

class CodeBERTModified():
  def __init__(self, my_pretrained_model):
    super(CodeBERTModified,self).__init__()
    self.pretrained = my_pretrained_model
  
  def forward(self,input_ids=None):
    outputs = self.pretrained(input_ids)
    out = outputs[1]
    return out

model = CodeBERTModified(my_pretrained_model=pretrained)

optimizer = torch.optim.Adam(model,lr=learning_rate)

for epoch in range(epoch_num):
  print("======Epoch%d======",epoch)
  print("======Traing=======")

  #cuda memory error
  gc.collect()
  torch.cuda.empty_cache()

  total_train_loss = 0
  model.train()

  for step, batch in enumerate(encoded_dataset):
    input1 = batch['code1']

#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

'''
args = TrainingArguments("test", save_strategy="epoch",evaluation_strategy="epoch",logging_strategy="epoch", 
                         learning_rate=1e-5,per_device_train_batch_size=batch_size,
                        per_device_eval_batch_size=batch_size,num_train_epochs=epoch_num,weight_decay=0.01,
                         do_train=True,do_eval=True,metric_for_best_model=metric_name,load_best_model_at_end=True)

trainer = Trainer(model,args,train_dataset=encoded_dataset['train'],eval_dataset=encoded_dataset['test'],
                  tokenizer=tokenizer, compute_metrics=compute_metrics, data_collator=data_collator)
'''

#trainer.train()

#trainer.evaluate()

TEST = "test.csv"
SAMPLE = "sample_submission.csv"

test_dataset = load_dataset("csv",data_files = TEST)['train']
test_dataset = test_dataset.map(tokenized,remove_columns=['code1','code2'])

predictions = trainer.predict(test_dataset)

df = pd.read_csv(SAMPLE)
df['similar'] = np.argmax(predictions.predictions,axis=-1)
df.to_csv('submission.csv',index=False)

'''
metric = load_metric("glue","qnli")

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return metric.compute(predictions=predictions, references=labels)

metric_name = "accuracy"
'''