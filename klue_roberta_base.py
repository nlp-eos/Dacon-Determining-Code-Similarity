# -*- coding: utf-8 -*-
"""klue-roberta base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QsENuwTOEIoYpR-nT5CR5dv3usUH7WK
"""

# Preprocess DATA
import pandas as pd
import re

data = pd.read_csv('sample_train.csv', encoding = 'utf-8')
print(type(data))
df = pd.DataFrame(data)
print(df)

#preprocess_df = df.replace(re.compile('(^import.*|^from.*)',re.MULTILINE),"",regex=True) #import,from 없애기
preprocess_df = df.replace(re.compile('(#.*)', re.MULTILINE),"",regex=True) #주석 한 줄
preprocess_df = preprocess_df.replace(re.compile('[\'\"]{3}.*?[\'\"]{3}', re.DOTALL),"",regex=True) #주석 여러줄
preprocess_df = preprocess_df.replace(re.compile('[\n]{2,}', re.MULTILINE),"\n",regex=True) #다중개행 한번으로
preprocess_df = preprocess_df.replace(re.compile('[ ]{4}', re.MULTILINE),"\t",regex=True) #tab 변환
preprocess_df = preprocess_df.replace(re.compile('[ ]{1,3}', re.MULTILINE)," ",regex=True) #공백 여러개 변환

print(preprocess_df)

preprocess_df.to_csv('preprocess.csv')

!pip install transformers

#Define Model

from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("klue/roberta-base", num_labels=1)
tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")

batch_size = 32
epoch_num = 5
MAX_LEN = 256

!pip install transformers datasets

from datasets import load_dataset
dataset = load_dataset("csv",data_files="preprocess.csv")['train']
print(dataset)

print(dataset['code1'][1])
print(dataset['code2'][1])

tokenizer(dataset['code1'][1],dataset['code2'][1])

#Tokenize

#preprocess_df['tokenized1'] = df.apply(lambda row: tokenizer.to(row['sentences']), axis=1)

def tokenized(examples):
  return tokenizer(examples['code1'],examples['code2'], padding=True, max_length=MAX_LEN,truncation=True, return_token_type_ids=False)
#print(preprocess_df.map(tokenized(preprocess_df)))

encoded_dataset = dataset.map(tokenized,remove_columns=['Unnamed: 0','code1','code2'],batched=True)
print(encoded_dataset)
#preprocess_df['tokenized1'] = df.apply(lambda row: tokenized(preprocess_df['code1'][:5]), axis=1)
#print(preprocess_df)
#print(preprocess_df['tokenized1'][0])
#print(preprocess_df['tokenized1'][1])

encoded_dataset=encoded_dataset.rename_column(original_column_name='similar',new_column_name='labels')
print(encoded_dataset)

encoded_dataset = encoded_dataset.train_test_split(0.2)

import numpy as np
from datasets import load_metric
from transformers import TrainingArguments, Trainer
import logging
import sklearn.metrics as metric

metric = load_metric("glue","qnli")

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return metric.compute(predictions=predictions, references=labels)

metric_name = "accuracy"

args = TrainingArguments("test", save_strategy="epoch",evaluation_strategy="epoch",logging_strategy="epoch", 
                         learning_rate=2e-5,per_device_train_batch_size=batch_size,
                        per_device_eval_batch_size=batch_size,num_train_epochs=10,weight_decay=0.01,
                         do_train=True,do_eval=True,metric_for_best_model=metric_name,load_best_model_at_end=True)

#test 
fake_preds = np.random.randint(0, 2, size=(64,))
fake_labels = np.random.randint(0, 2, size=(64,))
fake_preds, fake_labels
metric.compute(predictions=fake_preds, references=fake_labels)

#model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["acc"])
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(model,args,train_dataset=encoded_dataset['train'],eval_dataset=encoded_dataset['test'],
                  tokenizer=tokenizer, compute_metrics=compute_metrics, data_collator=data_collator)

import torch,gc

gc.collect()
torch.cuda.empty_cache()

trainer.train()

trainer.evaluate()

TEST = "test.csv"
SAMPLE = "sample_submission.csv"

test_dataset = load_dataset("csv",data_files = TEST)['train']
test_dataset = test_dataset.map(tokenized,remove_columns=['code1','code2'])

predictions = trainer.predict(test_dataset)

df = pd.read_csv(SAMPLE)
df['similar'] = np.argmax(predictions.predictions,axis=-1)
df.to_csv('submission.csv',index=False)
