# Dacon 2022 ì½”ë“œ ìœ ì‚¬ì„± íŒë‹¨ AI ê²½ì§„ëŒ€íšŒ 
ëŒ€íšŒ ë§í¬: https://dacon.io/competitions/official/235900/overview/description

## ì½”ë“œ ì„¤ëª…
https://dacon.io/codeshare/5159

## hyperparameter

- model: klue/roberta-base
  1. epoch_num=5, label_num=2, learning_rate=2e-5,train_test_split=0.2 â¡ï¸ accuracy:0.97, public score:0.7924689297
  2. epoch_num=10, label_num=2, learning_rate=2e-5,train_test_split=0.2 â¡ï¸ accuracy:0.97, public score:0.7943424226
  3. epoch_num=10, label_num=2, learning_rate=1e-5,train_test_split=0.1, seed=100 â¡ï¸ accuracy:0.97, public score:0.8014654053  
  
  ğŸ“Œ label_num=1ë¡œ í•  ê²½ìš° RuntimeError: Found dtype Long but expected Float ë°œìƒ (ì°¸ê³ : https://stackoverflow.com/questions/70490710/runtimeerror-found-dtype-long-but-expected-float-when-fine-tuning-using-trainer)   
  ğŸ“Œ klue/roberta-largeë„ í•´ë³´ë©´ ì¢‹ì„ë“¯   
  ğŸ“Œ í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ê°‘ìê¸° accuracy í™• ë‚®ì•„ì§€ëŠ” ê²½ìš°ê°€ ê½¤ ìˆëŠ”ë° ì´ëŸ´ ë•Œë§ˆë‹¤ í•™ìŠµë¥  ì¤„ì´ëŠ” ë°©ì‹ ë„ì…?!   
  -> klue ê²½ìš° í•œêµ­ì–´ì— ëŒ€í•œ pretrained model ì´ê¸° ë•Œë¬¸ì— codeì™€ëŠ” ë§ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨
***
- model: GraphCodeBERT  
  1. epoch_num=8, label_num=2, learning_rate=2e-5,train_test_split=0.1, MAX_LEN = 256, batch_size = 16, train_dataset = 46000(sample_train.csv+custom 27000) â¡ï¸ accuracy:0.975129, public score: 0.9478760898
  2. epoch_num=7, label_num=2, learning_rate=2e-5,train_test_split=0.1, MAX_LEN = 256, batch_size = 16, train_dataset = pretrained model(1.) + 45088(custom) â¡ï¸ accuracy:0.979596, public score: 0.9552958635

  ğŸ“Œ MAX_LEN = 512 / batch_size = 32ì¼ ê²½ìš° RuntimeError: CUDA out of memory   
  -> í•˜ë“œì›¨ì–´ìƒ MAX_LEN = 512ì´ ê°€ëŠ¥í•´ì§„ë‹¤ë©´ model_2ì—ì„œ scoreê°€ ë” ì˜¤ë¥¼ ìˆ˜ ìˆì„ ë“¯

***
- model: CodeBERT     
  1. ê¸°ì¡´ klue/roberta-base fine-tuning ì‹œí‚¤ëŠ” ëª¨ë¸ì—ì„œ ëª¨ë¸ë§Œ ë°”ê¿ˆ   
   model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16 â¡ï¸ accuracy: 0.97, public score:0.8968280467
  2. 6/8 model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16, optimizer=AdamW(weight_decay=0.0), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=5), metric=accuarcy â¡ï¸ accuracy: 0.97, public score:0.8627898349
  3. 6/9 model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16, optimizer=AdamW(weight_decay=0.1), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=5), trainset=new_trainset  â¡ï¸ accuracy: 0.96, public score: 0.9345019477
  4. 6/10 model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16, optimizer=AdamW(weight_decay=0.1), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=5, num_training_steps=5), trainset=new_trainset(40000), +preprocess(train_set) â¡ï¸ accuracy: 0.96, pulbic score: 0.9528287887
  5. 6/10 model: microsoft/codeBERT-base, learning_rate = 2e-5, MAX_LEN = 512, epoch_num = 7, batch_size = 16, optimizer=AdamW(weight_decay=0.1), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=5), trainset=new_trainset(45088), +preprocess(train_set) â¡ï¸ accuracy: 0.97, public score: 0.9553515118 â­ï¸ìµœì¢… submission

## ê²°ê³¼
- final code: microsoft-codeBERT/submission8
- private score: 0.95526 (43/337, 13%)

## ê°œì„  ë°©ì•ˆ
- sentence BERT ì‚¬ìš©
- MLM: DOBF ë°©ì‹ ì‚¬ìš©
- ì¢€ ë” ë§ì€ ì‹œë„ë¡œ ì ì ˆí•œ hyperparameter ì°¾ê¸°
- BM25 ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëª¨ë¸ì— ìµœì í™”ëœ í° ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ì…‹ êµ¬ì¶•
