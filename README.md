# Dacon 2022 ì½”ë“œ ìœ ì‚¬ì„± íŒë‹¨ NLP
### hyperparameter

- model: klue/roberta-base
  1. epoch_num=5, label_num=2, learning_rate=2e-5,train_test_split=0.2 â¡ï¸ accuracy:0.97, public score:0.7924689297
  2. epoch_num=10, label_num=2, learning_rate=2e-5,train_test_split=0.2 â¡ï¸ accuracy:0.97, public score:0.7943424226
  3. epoch_num=10, label_num=2, learning_rate=1e-5,train_test_split=0.1, seed=100 â¡ï¸ accuracy:0.97, public score:0.8014654053  
  
  ğŸ“Œ label_num=1ë¡œ í•  ê²½ìš° RuntimeError: Found dtype Long but expected Float ë°œìƒ (ì°¸ê³ : https://stackoverflow.com/questions/70490710/runtimeerror-found-dtype-long-but-expected-float-when-fine-tuning-using-trainer)   
  ğŸ“Œ klue/roberta-largeë„ í•´ë³´ë©´ ì¢‹ì„ë“¯   
  ğŸ“Œ í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ê°‘ìê¸° accuracy í™• ë‚®ì•„ì§€ëŠ” ê²½ìš°ê°€ ê½¤ ìˆëŠ”ë° ì´ëŸ´ ë•Œë§ˆë‹¤ í•™ìŠµë¥  ì¤„ì´ëŠ” ë°©ì‹ ë„ì…?!   
  -> klue ê²½ìš° í•œêµ­ì–´ì— ëŒ€í•œ pretrained model ì´ê¸° ë•Œë¬¸ì— codeì™€ëŠ” ë§ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨
***
- model: GraphCodeBERT  
  ğŸ“Œ default : label_num=2, learning_rate=2e-5,train_test_split=0.1, MAX_LEN = 256, batch_size = 16
  1. epoch_num=8, train_dataset = 46000?(sample_train.csv+custom 27000) â¡ï¸ accuracy:0.975129, public score:
  2. epoch_num=7, train_dataset = 45088(custom) â¡ï¸ accuracy:0., public score:  

  ğŸ“Œ MAX_LEN = 512 / batch_size = 32ì¼ ê²½ìš° RuntimeError: CUDA out of memory

***
- model: CodeBERT     
 -> codeBERT ê²½ìš° ml-pl pairì— ëŒ€í•œ ëª¨ë¸, plì— ëŒ€í•´ì„œë§Œ embedding -> cosineSimilarityë¥¼ í†µí•œ sequenceClassfication -> loss -> í•™ìŠµ??
  1. ê¸°ì¡´ klue/roberta-base fine-tuning ì‹œí‚¤ëŠ” ëª¨ë¸ì—ì„œ ëª¨ë¸ë§Œ ë°”ê¿ˆ   
   model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16 â¡ï¸ accuracy: 0.97, public score:0.8968280467
  2. 6/8 model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16, optimizer=AdamW(weight_decay=0.0), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=5), metric=accuarcy â¡ï¸ accuracy: 0.97, public score:0.8627898349
  3. 6/9 model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16, optimizer=AdamW(weight_decay=0.1), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=5), trainset=new_trainset  â¡ï¸ accuracy: 0.96, public score: 0.9345019477
  4. 6/10 model: microsoft/codeBERT-base, learning_rate = 1e-5, MAX_LEN = 512, epoch_num = 10, batch_size = 16, optimizer=AdamW(weight_decay=0.1), scheduler = transformers.get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=5, num_training_steps=5), trainset=new_trainset(40000), +preprocess(train_set) â¡ï¸ accuracy: 0.96, pulbic score: 0.9528287887â­ï¸

***
### ê°œì„  ë°©ì•ˆ
- sentence BERT ì‚¬ìš©
- MLM: DOBF ë°©ì‹ ì‚¬ìš©
- ì¢€ ë” ë§ì€ ì‹œë„ë¡œ ì ì ˆí•œ hyperparameter ì°¾ê¸°
- B25 ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëª¨ë¸ì— ìµœì í™”ëœ í° ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ì…‹ êµ¬ì¶•
