# -*- coding: utf-8 -*-
"""GraphCodeBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_K59zx5wijjsx0x8vzOthWX_97V7w7s
"""

from google.colab import drive
drive.mount('/content/drive')

"""##데이터 다운로드"""

import os
from tqdm import tqdm

# Commented out IPython magic to ensure Python compatibility.
# 데이터 압축 풀기
# %cd /content/drive/MyDrive/determining_code_similarity_AI_competition/data
!unzip -qq "/content/drive/MyDrive/open.zip"

# 데이터 개수 확인
import os

def get_files_count(folder_path):
	dirListing = os.listdir(folder_path)
	return len(dirListing)
	
dir_path = "/content/drive/MyDrive/determining_code_similarity_AI_competition/data/code"
cnt = 0
fcnt = {}

for (root, directories, files) in os.walk(dir_path):
  for d in directories:
    f = get_files_count(os.path.join(root, d))
    fcnt[d] = f
    cnt += f

print(fcnt)
print("전체 코드 파일 수: ",cnt)

"""##데이터 전처리"""

import os
import pandas as pd
import re

"""####sample_train.csv 전처리"""

# sample_train.csv 처리
file_path = '/content/drive/MyDrive/determining_code_similarity_AI_competition/data/sample_train.csv'
train = pd.read_csv(file_path, encoding='latin1')
train.info()

train.head()

# train = train.replace(re.compile('(^import.*|^from.*)',re.MULTILINE),"",regex=True) #import,from 없애기
train = train.replace(re.compile('(#.*)', re.MULTILINE),"",regex=True) #주석 한 줄
train = train.replace(re.compile('[\'\"]{3}.*?[\'\"]{3}', re.DOTALL),"",regex=True) #주석 여러줄
train = train.replace(re.compile('[\n]{2,}', re.MULTILINE),"\n",regex=True) #다중개행 한번으로
train = train.replace(re.compile('[ ]{4}', re.MULTILINE),"\t",regex=True) #tab 변환
train = train.replace(re.compile('[ ]{1,3}', re.MULTILINE)," ",regex=True) #공백 여러개 변환
train

"""##모델 구성"""

!pip install transformers
!pip install datasets

import numpy as np
import torch
import datasets
from datasets import load_dataset, load_metric, Dataset
from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer

# train, valid 데이터 분리
from sklearn.model_selection import train_test_split

train_df, valid_df, train_label, valid_label = train_test_split(
        train,
        train['similar'],
        random_state=1234,
        test_size=0.2,
        stratify=train['similar'],
        )

train_df.head()

# DatasetDict 타입으로 변환
train_dataset = Dataset.from_dict(train_df)
valid_dataset = Dataset.from_dict(valid_df)
dataset = datasets.DatasetDict({"train":train_dataset,"valid":valid_dataset})
dataset

batch_size = 32 ########3
metric = load_metric("glue", "qnli") ##########3
metric_name = "accuracy"

tokenizer = RobertaTokenizer.from_pretrained("microsoft/graphcodebert-base")
tokenizer.truncation_side = 'left'
# MAX_LEN = 512

def tokenize_function(sample):
    return tokenizer(
        sample['code1'],
        sample['code2'],
        padding=True,
        truncation='longest_first',
        return_token_type_ids=True,
        return_attention_mask=True,
        return_length=True,
    )

# 함수 테스트
tokenize_function(dataset["train"][:5])

# 토큰화
encoded_dataset = dataset.map(tokenize_function, batched=True)

import pickle
 
with open("/content/drive/MyDrive/determining_code_similarity_AI_competition/data/encoded_dataset", "wb" ) as file:
  pickle.dump(encoded_dataset, file)

with open("/content/drive/MyDrive/determining_code_similarity_AI_competition/data/encoded_dataset", "rb" ) as file:
  loaded_data = pickle.load(file)
  print(loaded_data)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

# binary classification으로 SequenceClassification 모델 로드
num_labels = 1
model = RobertaForSequenceClassification.from_pretrained("microsoft/graphcodebert-base", num_labels=num_labels)

args = TrainingArguments(
    "test_GraphCodeBERT",
    evaluation_strategy="epoch", save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
)

import torch,gc
gc.collect()
torch.cuda.empty_cache()

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["valid"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

!pip install knockknock

gc.collect()
torch.cuda.empty_cache()

from knockknock import discord_sender

@discord_sender(webhook_url="https://discordapp.com/api/webhooks/981434503673286666/SPt7eHcgTwA-1JCfBJNoKkLCigm3LM4ll3DJxSH9Ym4uQNUsr-PalYs38YtSabWz1j4U")
def train_fn():
  gc.collect()
  torch.cuda.empty_cache()
  trainer.train()
  gc.collect()
  torch.cuda.empty_cache()
  pass

train_fn()